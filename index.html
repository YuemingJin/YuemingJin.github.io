<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yueming Jin, Jin Yueming, Yueming, BME, ECE, NUS, National University of Singapore, CSE, CUHK, The Chinese University of Hong Kong, WEISS, UCL, University College London"> 
<meta name="description" content="Yueming Jin&#39;s home page">
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Yueming Jin&#39;s Homepage</title>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
</head>
<body>
<div id="layout-content" style="margin-top:25px">



<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Yueming Jin</h1><h1>
				</h1></div>

				<h3>Assistant Professor</h3>
				<p>
					Department of Biomedical Engineering <br>
					Department of Electrical and Computer Engineering <br>
					National University of Singapore (NUS) <br>
					<br>
					Office: Block E7, #05-01H, NUS, Singapore 119276 <br>
					Email: ymjin AT nus.edu.sg
				</p>
				<p> <a href="https://scholar.google.com/citations?user=s_kbB4oAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/jinlab-imvr"><img src="./pic/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://orcid.org/0000-0003-3775-3877"><img src="./pic/orcid.jpeg" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.researchgate.net/profile/Yueming_Jin"><img src="./pic/rg.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://twitter.com/JinYueming"><img src="./pic/twitter.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://www.linkedin.com/in/yueming-jin-51097a130"><img src="./pic/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
					<!--
					<a href="https://zh-cn.facebook.com/people/Lequan-Yu/100003696557697"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
					-->
				</p>
			</td>
			<td>
				<img src="./pic/skylarJY2.jpg" border="0" width="200"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>
<!--[<a href="./CV-JinYueming.pdf">CV</a>]-->
<h2>Biography </h2>
<p>
	I am an Assistant Professor at <a href="https://cde.nus.edu.sg/bme/">Department of Biomedical Engineering</a> and <a href="https://cde.nus.edu.sg/ece/">Department of Electrical and Computer Engineering</a> at <a href="https://nus.edu.sg/">National University of Singapore</a>. Before joining NUS, I was a senior research fellow at the <a href="https://www.ucl.ac.uk/computer-science/">Department of Computer Science</a>, <a href="https://www.ucl.ac.uk/">University College London</a>, working with <a href="https://www.ucl.ac.uk/surgical-robot-vision/people/danail-stoyanov">Prof. Danail Stoyanov</a>. I received my Ph.D. degree in the <a href="http://www.cse.cuhk.edu.hk/">Department of Computer Science and Engineering</a>, <a href="http://www.cuhk.edu.hk/">The Chinese University of Hong Kong</a>, advised by <a href="http://www.cse.cuhk.edu.hk/~pheng" target="_blank">Prof. Pheng-Ann Heng</a> and <a href="http://www.cse.cuhk.edu.hk/~cwfu" target="_blank">Prof. Chi-Wing Fu</a>. I am listed at Forbes 30 Under 30 Asia, Class 2024.
	<!--Previously, I received the B. Eng degree from Biomedical Engineering in <a href="https://www.neu.edu.cn/">Northeastern University</a> in 2015.</p>-->
</p>

<p>My research interests include artificial intelligence and its applications on healthcare, with an emphasized application to medical image computing and robotic surgical data science. I recently work on multi-modality medical AI, agentic workflow with MLLM, resource efficient learning, and smart agent for robotic surgery with 3D reconstruction.</p>
<!--<p>My research interests include medical image computing, robotic video perception and artificial intelligence. Specifically, I am dedicated to developing learning algorithms for complex time-series medical data analysis, applied primarily to endoscopy and robotic surgery. I recently work on data efficient learning to minimize human annotation.</p>-->

<p style="color:red;"> <b>*Opening!*</b> The lab is looking for PhD/Postdoc/RA/Interns/Visiting students, who are interested in machine learning, medical image analysis, surgical 2D/3D vision, surgical robotic intelligence. Students with strong self-motivation and publication record are preferred. Welcome to email me after reading the <b><a href="./files/JobDescription_1221.pdf">details</a></b>. </p>

<p style="color:red;"> <b>*New*</b> Our lab recently has some openings for Postdoc and 2025 Summer Intern, with the following directions: 1) Vision-language multi-modality learning; 2) 2D/3D image generation; 3) Embodied AI for robotic surgery intelligence. Welcome to email me!

<p>If you are an NUS student interested in doing research with me, also feel free to drop me an email!</p>


<!--<p style="color:red;">
	<a href="https://www.ucl.ac.uk/surgical-robot-vision/"> SRV group, WEISS</a> is looking for self-motivated summer interns (2022), to work on the research areas of robotic surgery intelligence and surgical 3D vision. Welcome to drop me an email with your CV. Remote collaboration is also welcome.
</p>-->

<h2>News</h2>
<div style="height: 280px; overflow: auto;">
<ul>
	<li>
		[06/2025] Our paper <a href="https://arxiv.org/pdf/2408.03208" target="_blank">PFedSIS</a> about FL for surgical instrument segmentation <a href="https://www.embs.org/tbme/articles/personalizing-federated-instrument-segmentation-with-visual-trait-priors-in-robotic-surgery/" target="_blank">is <b>featured</b> by IEEE TBME, June</a>! One paper about LLM for continual surgical VQA accepted to IEEE TMI.
	</li>
	<li>
		[05/2025] Paper about <a href="https://www.arxiv.org/abs/2505.08581" target="_blank">Referring Surgical Video Segmentation</a> is early accepted to MICCAI 2025; Two papers about Agentic Reasoning and Medical RAG accepted to ACL main 2025.
	</li>
	<li>
		[05/2025] To serve as Program Co-chair of IPCAI 2026 and MIDL 2026.
	</li>
	<li>
		[05/2025] One paper accepted to ICML 2025 with <b>spotlight</b>.
	</li>
	<li>
		[04/2025] Call for Papers - We will host two workshops in MICCAI'25: #1 <a href="https://sites.google.com/view/miccai-2025-colas?usp=sharing" target="_blank">COLAS:COLlaborative Intelligence and Autonomy in Image-guided Surgery</a>; #2 <a href="https://haic-miccai.github.io/#/" target="_blank">HAIC:Human-AI Collaboration</a>.
	</li>
	<li>
		[02/2025] One paper about fedarated continual learning for medical image segmentation is accepted to IPMI 2025.
	</li>
	<li>
		[01/2025] Serve as Area Chair of MICCAI 2025. 
	</li>
	<li>
		[01/2025] The surgical error detection method <a href="https://github.com/jinlab-imvr/Chain-of-Gesture" target="_blank">COG</a> will be presented at ICRA 2025; One paper about fedarated learning on surgical instrument segmentation is accepted to TBME.
	</li>
	<li>
		[10/2024] Our AAAI Bridge Program on <a href="https://sites.google.com/view/aimedhealth-aaai/home" target="_blank">AI for Medicine and Healthcare</a> will be held on February 25, 2025.
	</li>
	<li>
		[10/2024] One paper about surgical error detection accepted to IEEE RA-L; Our P2IFL Challenge paper on pre-operative CT to intra-operative surgical video registration accepted to Medical Image Analysis.
	</li>
	<li>
		[09/2024] Surgical SAM 2 is accepted to NeurIPS AIM-FM workshop; One paper about semi-supervised continual segmentation accepted to IEEE TMI.
	</li>
	<li>
		[08/2024] One paper about multimodal scene graph generation in OR accepted to IEEE TMI; one co-authored paper accepted to Science Bulletin.
	</li>
	<li>
		[06/2024] Invited talk about intelligent surgical vision at VALSE Webinar, China <a href="https://www.bilibili.com/video/BV1hw4m1Y7pf/?spm_id_from=333.1007.top_right_bar_window_dynamic.content.click&vd_source=fd5f237755b930fc4af6d6bf0bb3b9bc" target="_blank">[Record in Chinese]</a>. 
	</li>
	<li>
		[06/2024] One paper about video-text learning in surgery accepted to IEEE TMI; one paper about multimodal MRI fusion accepted to ECCV 2024; four papers accepted to MICCAI 2024.
	</li>
	<li>
		[05/2024] Excited to share that I have been selected in <a href="https://www.forbes.com/profile/jin-yueming/?list=30under30-asia-healthcare-science/&sh=263521c16612" target="_blank">Forbes 30 Under 30 Asia List</a>, Class 2024. 
	</li>
	<li>
		[05/2024] Paper about Gaussian Splitting for Fast Surgical Scene Reconstruction is early accepted to MICCAI 2024. 
	</li>
	<li>
		[05/2024] Invited talk at <a href="https://sites.google.com/view/icra24-c4sr/home?authuser=0" target="_blank">ICRA C4SR+ workshop</a>.
	</li>
	<li>
		[04/2024] One paper accepted to IEEE TMI and one paper accepted to CVPR 2024. 
	</li>
	<li>
		[01/2024] Serve as Area Chair of MICCAI 2024. 
	</li>
	<li>
		[01/2024] One paper about LLM-assisted continual learning for surgical VQA is accepted to ICRA 2024.
	</li>
	<li>
		[12/2023] Two papers are accepted to AAAI 2024.
	</li>
	<li>
		[12/2023] Serve as Associate Editor of <a href="https://www.melba-journal.org/" target="_blank">Machine Learning for Biomedical Imaging Journal (MELBA)</a>.
	</li>
	<li>
		[11/2023] Invited talk at BII-AI3 Special Interest Group of Digital Health, A*STAR.
	</li>
	<li>
		[09/2023] One paper is accepted to Nature Communications.
	</li>
	<li>
		[08/2023] Invited talk at Cancer Neuroscience Symposium, organized by National Neuroscience Institute (NNI), SingHealth.
	</li>
	<li>
		[07/2023] One paper accepted to IEEE TMI and one paper accepted to ACM MM 2023.
	</li>
	<li>
		[06/2023] Four papers are accepted to MICCAI 2023 (Two early accepted).
	</li>
	<li>
		[06/2023] We are organizing a Special Issue on <a href="https://www.embs.org/jbhi/wp-content/uploads/sites/18/2023/05/Trustworthy-20230401_CallforPapers_TMLH.pdf" target="_blank">Trustworthy Machine Learning for Health Informatics</a> in IEEE-JBHI and welcome to submit your manuscripts.
	</li>
	<li>
		[06/2023] Our 2nd MICCAI Workshop on Cancer Prevention through Early Detection <a href="https://caption-workshop.github.io/miccai2023/" target="_blank">(CaPTion)</a> call for papers.
	</li>
	<li>
		[05/2023] Our paper on joint workflow recognition and anticipation won <a href="http://www.miccai.org/about-miccai/awards/ijcars-best-paper-award/" target="_blank">1st Prize of IJCARS-MICCAI 2021 Best Paper Award</a>!
	</li>
	<li>
		[01/2023] Serve as Area Chair of MICCAI 2023. 
	</li>
	<li>
		[01/2023] Two papers are accepted to ICRA and RAL about surgical skill assessment and report generation.
	</li>
	<!--
	<li>
		[01/2023] Our 1st ICLR Workshop on Trustworthy Machine Learning in Healthcare <a href="https://sites.google.com/view/tml4h2023/home?authuser=0" target="_blank">(TML4H)</a> call for papers.
	</li> -->
	<li>
		[11/2022] Serve as Area Chair of IPCAI 2023.
	</li>
	<!--
	<li>
		[10/2022] Our IEEE-RAS <a href="https://softroboticsconference.org/" target="_blank"> RoboSoft 2023</a> on “Soft Robotics for Sustainable Development” call for papers.
	</li> -->
	<li>
		[09/2022] Our invited paper on joint workflow recognition and anticipation is accepted to IJCARS.
	</li>
	<li>
		[07/2022] Invited talk at <a href="https://www.miua2022.com/WiMIUA" target="_blank">WiMIUA</a>, University of Cambridge.
	</li>
	<li>
		[07/2022] One paper on personalized FL on medical image segmentation is accepted to ECCV.
	</li>
	<li>
		[06/2022] One paper is accepted to IROS.
	</li>
	<li>
		[05/2022] One paper on surgical scene segmentation is accepted to IEEE TMI.
	</li>
	<!--
	<li>
		[05/2022] Welcome to participate in our MICCAI Challenge on Simulated Colonoscopy data for 3D <a href="https://www.synapse.org/#!Synapse:syn28548633/wiki/" target="_blank">(SimCol)</a>.
	</li>
	<li>
		[05/2022] Our 1st MICCAI Workshop on Cancer Prevention through Early Detection <a href="https://caption-workshop.github.io/" target="_blank">(CaPTion)</a> call for papers.
	</li>
	<li>
		[05/2022] Welcome to participate in our MICCAI Challenge on Preoperative to Intraoperative Laparoscopy Fusion <a href="https://p2ilf.grand-challenge.org/P2ILF/" target="_blank">(P2ILF)</a>.
	</li>
-->
	<li>
		[01/2022] One paper on Transformer for tool tracking of robotic surgery was accepted to ICRA 2022.
	</li>
	<li>
		[12/2021] One paper on landmark detection in ESD surgery was accepted to MedIA.
	</li>
	<li>
		[10/2021] We won the first place of 2021 MICCAI <a href="https://www.synapse.org/#!Synapse:syn25127311/wiki/" target="_blank">SimSurgSkill-Skill Assessment Challenge</a>. Congrats to Ema and Dimi.
	</li>
	<li>
		[09/2021] One paper on adaptive segmentation from general videos to surgical videos was accepted to MedIA.
	</li>
	<li>
		[07/2021] One paper on video corresponding learning was accepted to ICCV 2021.
	</li>
	<li>
		[06/2021] One paper on semi-supervised surgical workflow recognition was accepted to MedIA.
	</li>
	<li>
		[06/2021] Two papers on future frame prediction and DA in robotic action recognition were accepted to IROS 2021.
	</li>
	<li>
		[06/2021] Our paper on multi-modality graph learning won ICRA 2021 Best Paper Award in Medical Robotics!
	</li>
	<li>
		[05/2021] Three papers were <b>early accepted</b> to MICCAI 2021 with one <b>oral</b>.
	</li>
	<li>
		[05/2021] I was invited as the Session Chair of ICRA 2021.
	</li>
	<li>
		[03/2021] Our paper on relating temporal cues for workflow recognition is accepted to IEEE TMI.
	</li>
	<li>
		[03/2021] Two papers accepted to ICRA 2021.
	</li>
	<li>
		[02/2021] Our paper on future scene prediction for robotic surgery is accepted to IPMI 2021 with <b>oral</b>.
	</li>
	<li>
		[01/2021] Our ROBUST-MIS 2019 challenge paper is accepted to MedIA.
	</li>
	<!--
	<li>
		[07/2020] Two papers accepted to MICCAI 2020. One paper accepted to IROS 2020.
	</li>
	<li>
		[03/2020] One paper accepted to ICRA 2020. One paper accepted to IPCAI&IJCARS 2020.
	</li>
	<li>
		[10/2019] Our team won the first place of 2019 MICCAI <a href="https://www.synapse.org/#!Synapse:syn18779624/wiki/597117" target="_blank">ROBUST-MIS Challenge on Multi-instance Segmentation</a>.
	</li>
	<li>
		[10/2019] Our team won the first place of 2019 MICCAI <a href="https://www.synapse.org/#!Synapse:syn18824884/wiki/599131" target="_blank">EndoVis Surgical Workflow Challenge on Phase Segmentation</a>.
	</li>
	<li>
		[07/2019] Two papers accepted to MICCAI 2019 with one <b>oral</b> (3%).
	</li>

	<li>
		[06/2019] I finished my PhD defense on June 12th, 2019. 
	</li>
-->
</ul>
</div>


<h2>Selected Publications [Find full list at <a href="https://scholar.google.com/citations?user=s_kbB4oAAAAJ&hl=en">Google Scholar</a>]</h2>
	<!-- <p><font face="Arial" size="4"><b>2023</b></font></p> -->
	<ul>
		<li>
			<a href="https://arxiv.org/pdf/2503.18968" target="_blank">MedAgent-Pro: Towards Evidence-based Multi-modal Medical Diagnosis via Reasoning Agentic Workflow</a><br>
			Ziyue Wang, Junde Wu, Chang Han Low, <b>Yueming Jin</b> <br>
			<font color="blue">(We present an evidence-based reasoning agentic system with both quantitative and qualitative evidence for explainable medical diagnosis)</font><br>			
			<em>Preprint</em>, 2025. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/MedAgent-Pro" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2503.10265" target="_blank">SurgRAW: Multi-Agent Workflow with Chain-of-Thought Reasoning for Surgical Intelligence</a><br>
			Chang Han Low, Ziyue Wang, Tianyi Zhang, Zhitao Zeng, Zhu Zhuo, Evangelos B. Mazomenos, <b>Yueming Jin</b> <br>
			<font color="blue">(We present a CoT-driven multi-agent framework for accurate and interpretable analysis of surgical vision tasks)</font><br>			
			<em>Preprint</em>, 2025. <br> 
		</li>
			<p style="margin-top:3px">

			</p>

		<li>
			<a href="https://arxiv.org/abs/2409.01003" target="_blank">Free-DyGS: Camera-Pose-Free Scene Reconstruction for Dynamic Surgical Videos with Gaussian Splatting</a><br>
			Qian Li, Shuojue Yang, Daiyun Shen, Jimmy Bok Yan So, Jing Qin, <b>Yueming Jin</b> <br>
			<font color="blue">(We present a novel GS-based method for a challenging setup of surgical scene reconstruction -> free camera pose + highly dynamic scene with deformation)</font><br>			
			<em>Preprint</em>, 2025. <br> 
		</li>
			<p style="margin-top:3px">

			</p>

		<li>
			<a href="https://www.arxiv.org/abs/2505.08581" target="_blank">ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking</a><br>
			Haofeng Liu, Mingqi Gao, Xuxiao Luo, Ziyue Wang, Guanyi Qin, Junde Wu, <b>Yueming Jin</b> <br>		
			<em>Medical Image Computing and Computer Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2025. <i><p style="color: red; display: inline;">(Early accept)</p></i> <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/ReSurgSAM2" target="_blank">code</a>]
			</p>

		<li>
			<a href="" target="_blank">Automatically Identify and Rectify: Robust Deep Contrastive Multi-view Clustering in Noisy Scenarios</a><br>
			Xihong Yang, Siwei Wang, Fangdi Wang, Jiaqi Jin, Suyuan Liu, Yue Liu, En Zhu, Xinwang Liu, <b>Yueming Jin</b> <br>		
			<em> International Conference on Machine Learning</em>, (<i><b>ICML</b></i>) 2025. <i><p style="color: red; display: inline;">(Spotlight)</p></i> <br> 
		</li>
		<!--
		<li>
			<a href="https://arxiv.org/abs/2406.01987" target="_blank">Dealing with All-stage Missing Modality: Towards A Universal Model with Robust Reconstruction and Personalization</a><br>
			Yunpeng Zhao, Cheng Chen, Qing You Pang, Quanzheng Li, Carol Tang, Beng-Ti Ang, <b>Yueming Jin</b> <br>
			<font color="blue">(We present a universal model to tackle missing modality in both train and test stages for brain tumor segmentation)</font><br>			
			<em>Preprint</em>, 2024. <br> 
		</li>
			<p style="margin-top:3px">

			</p>
		-->
		<li>
			<a href="https://arxiv.org/abs/2503.18064" target="_blank">Dynamic Allocation Hypernetwork with Adaptive Model Recalibration for Federated Continual Learning</a><br>
			Xiaoming Qi, Jingyang Zhang, Huazhu Fu, Guanyu Yang, Shuo Li, <b>Yueming Jin</b> <br>
			<em>Information Processing in Medical Imaging</em> (<i><b>IPMI</b></i>), 2025. <i><p style="color: red; display: inline;">(Oral)</p></i>
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/FedDAH" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://www.sciencedirect.com/science/article/pii/S1361841525000945" target="_blank">Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation</a><br>
			Junde Wu, Ziyue Wang, Mingxuan Hong, Wei Ji, Huazhu Fu, Yanwu Xu, Min Xu, <b>Yueming Jin</b> <br>
			<font color="black">(We present a simple yet effective adaptation technique, to extend SAM to medical domain)</font><br>			
			<em>Medical Image Analysis</em>, 2025. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/Medical-SAM-Adapter" target="_blank">code</a>][<a href="https://huggingface.co/KidsWithTokens/Medical-Adapter-Zoo/tree/main" target="_blank">Medical Adapter Zoo</a>]
			</p>
		
		<li>
			<a href="https://arxiv.org/abs/2406.19217" target="_blank">Think Step by Step: Chain-of-Gesture Prompting for Error Detection in Robotic Surgical Videos</a><br>
			Zhimin Shao, Jialang Xu, Danail Stoyanov, Evangelos B Mazomenos, <b>Yueming Jin</b> <br>
			<em>IEEE Robotics and Automation Letters</em> (<i><b>RA-L</b></i>), 2024. <br>
			& <em>IEEE International Conference on Robotics and Automation</em> (<i><b>ICRA</b></i>), 2025.
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/Chain-of-Gesture" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/pdf/2408.03208" target="_blank">Personalizing Federated Instrument Segmentation With Visual Trait Priors in Robotic Surgery</a><br>
			Jialang Xu, Jiacheng Wang, Lequan Yu, Danail Stoyanov, <b>Yueming Jin†</b> , Evangelos B. Mazomenos† <br>
			<em>IEEE Transactions on Biomedical Engineering</em> (<i><b>TBME</b></i>), 2025. <br> 
			<font color="red">(TBME Featuring Paper in June)</font>
			</i>[<a href="https://www.embs.org/tbme/articles/personalizing-federated-instrument-segmentation-with-visual-trait-priors-in-robotic-surgery/" target="_blank">link</a>]
		
		</li>


		<li>
			<a href="https://arxiv.org/abs/2408.07931" target="_blank">Surgical SAM 2: Real-time segment anything in surgical video by efficient frame pruning</a><br>
			Haofeng Liu, Erli Zhang, Junde Wu, Mingxuan Hong, <b>Yueming Jin</b> <br>
			<em>Neural Information Processing Systems, Advancements In Medical Foundation Models</em>, 2024. 
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/Surgical-SAM-2" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2405.17835" target="_blank">Deform3DGS: Flexible Deformation for Fast Surgical Scene Reconstruction with Gaussian Splatting</a><br>
			Shuojue Yang, Qian Li, Daiyun Shen, Bingchen Gong, Qi Dou, <b>Yueming Jin</b> <br>
			<em>Medical Image Computing and Computer Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2024. <i><p style="color: red; display: inline;">(Early accept)</p></i>
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/Deform3DGS" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://link.springer.com/chapter/10.1007/978-3-031-72667-5_25" target="_blank">Energy-Induced Explicit Quantification for Multi-modality MRI Fusion</a><br>
			Xiaoming Qi, Yuan Zhang, Tong Wang, Guanyu Yang†, <b>Yueming Jin†</b>, Shuo Li <br>
			<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2024. 
		</li>

		<li>
			<a href="https://arxiv.org/abs/2402.14461" target="_blank">S^2Former-OR: Single-Stage Bimodal Transformer for Scene Graph Generation in OR</a><br>
			Jialun Pei, Diandian Guo, Jingyang Zhang, Manxi Lin, <b>Yueming Jin†</b>, Pheng-Ann Heng <br> 
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>IEEE TMI</b></i>), 2024. 
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/S2Former-OR" target="_blank">code</a>][<a href="https://rascalgdd.github.io/ORSGG/" target="_blank">project with reformatted annotation</a>]
			</p>

		<li>
			<a href="https://ieeexplore.ieee.org/abstract/document/10595513" target="_blank">Video-instrument synergistic network for referring video instrument segmentation in robotic surgery</a><br>
			Hongqiu Wang, Guang Yang, Shichen Zhang, Jing Qin, Yike Guo, Bo Xu, <b>Yueming Jin†</b>, Lei Zhu <br>
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>IEEE TMI</b></i>), 2024. 
		
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jinlab-imvr/RSVIS" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2307.11261" target="_blank">SimCol3D -- 3D Reconstruction during Colonoscopy Challenge</a><br>
			Anita Rau†, Sophia Bano†, <b>Yueming Jin†</b>, Pablo Azagra, Javier Morlana, Edward Sanderson, Bogdan J. Matuszewski, Jae Young Lee, Dong-Jae Lee, Erez Posner, Netanel Frank, Varshini Elangovan, Sista Raviteja, Zhengwen Li, Jiquan Liu, Seenivasan Lalithkumar, Mobarakol Islam, Hongliang Ren, José M.M. Montiel, Danail Stoyanov <br>
			<!-- <font color="blue">(Summarization report for 3D Reconstruction during Colonoscopy MICCAI 2022 Challenge)</font><br> -->			
			<em>Medical Image Analysis</em>, 2024. <br> 
		</li>
			<p style="margin-top:3px">

			</p>

		<li>
			<a href="https://arxiv.org/abs/2305.10300" target="_blank">One-Prompt to Segment All Medical Images</a><br>
			Junde Wu, Jiayuan Zhu, <b>Yueming Jin</b>, Min Xu  <br>		
			<em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<i><b>CVPR</b></i>), 2024. <br> 
		</li>
			<p style="margin-top:3px">
			[<a href="https://github.com/KidsWithTokens/one-prompt" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2402.16664" target="_blank">LLM-Assisted Multi-Teacher Continual Learning for Visual Question Answering in Robotic Surgery</a><br>
			Kexin Chen, Yuyang Du, Tao You, Mobarakol Islam, Ziyu Guo, <b>Yueming Jin†</b>, Guangyong Chen, Pheng-Ann Heng <br>		
			<em>IEEE International Conference on Robotics and Automation</em> (<i><b>ICRA</b></i>), 2024. <br> 
		</li>
			<p style="margin-top:3px">
			[<a href="./video/2024ICRA.mp4" target="_blank">Video demo</a>]
			</p>



		<li>
			<a href="https://ieeexplore.ieee.org/abstract/document/10508990" target="_blank">Calibrate the Inter-observer Segmentation Uncertainty via Diagnosis-first Principle</a><br>
			Junde Wu, Yu Zhang, Huihui Fang, Lixin Duan, Mingkui Tan, Weihua Yang, Chunhui Wang, Huiying Liu, <b>Yueming Jin†</b>, Yanwu Xu† <br>
			<!-- <font color="blue">(Summarization report for 3D Reconstruction during Colonoscopy MICCAI 2022 Challenge)</font><br> -->			
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>TMI</b></i>), 2024. <br> 
		</li>
			<p style="margin-top:3px">

			</p>

		<li>
			<a href="https://ojs.aaai.org/index.php/AAAI/article/view/28418" target="_blank">Medsegdiff-v2: Diffusion based medical image segmentation with transformer</a><br>
			Junde Wu, Wei Ji, Huazhu Fu, Min Xu, <b>Yueming Jin</b>, Yanwu Xu <br>
			<!-- <font color="blue">(Summarization report for 3D Reconstruction during Colonoscopy MICCAI 2022 Challenge)</font><br> -->			
			<em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (<i><b>AAAI</b></i>), 2024. <br> 
			<font color="red">(AAAI-24 Most Influential Paper)</font></br> 
		</li>
			<p style="margin-top:3px">
			[<a href="https://github.com/SuperMedIntel/MedSegDiff" target="_blank">code</a>][<a href="https://www.paperdigest.org/2024/09/most-influential-aaai-papers-2024-09/" target="_blank">link</a>]
			</p>

		<li>
			<a href="https://ieeexplore.ieee.org/abstract/document/10194959" target="_blank">FedDP: Dual Personalization in Federated Medical Image Segmentation</a><br>
			Jiacheng Wang, <b>Yueming Jin</b>, Danail Stoyanov, Liansheng Wang. <br>
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>TMI</b></i>), 2023. <br> 
		</li>
			<p style="margin-top:3px">

			</p>
		<li>
			<a href="https://ieeexplore.ieee.org/abstract/document/10037203" target="_blank">Keep Your Eye on the Best: Contrastive Regression Transformer for Skill Assessment in Robotic Surgery</a><br>
			Dimitrios Anastasiou, <b>Yueming Jin</b>, Danail Stoyanov, Evangelos Mazomenos. <br>
			<em>IEEE Robotics and Automation Letters</em> (<i><b>RA-L</b></i>), 2023. <br> 
		</li>
			<p style="margin-top:3px">

			</p>


	<li>
		<a href="https://link.springer.com/article/10.1007/s11548-022-02743-8" target="_blank">Trans-SVNet: Hybrid Embedding Aggregation Transformer for Surgical Workflow Analysis</a><br>
		<b>Yueming Jin</b>, Yonghao Long, Xiaojie Gao, Danail Stoyanov, Qi Dou, Pheng-Ann Heng. <br>
		<em>International Journal for Computer Assisted Radiology and Surgery</em> (<i><b>IJCARS</b></i>), 2022. <br>
		<font color="red">(1st Prize of Best Paper Award of IJCARS-MICCAI 2021)</font></br> 
	</li>
		<p style="margin-top:3px">
			[<a href="https://github.com/YuemingJin/Trans-SVNet_Journal" target="_blank">code</a>][<a href="http://www.miccai.org/news/2023/06/06/ijcars-miccai-2021-best-paper-awards-announced#:~:text=%E2%80%9CCongratulations%20to%20the%20authors%20on,awards%2C%20are%20presented%20each%20year." target="_blank">award</a>] [<a href="https://cde.nus.edu.sg/news-detail/asst-prof-jin-yueming-wins-first-prize-for-ai-in-surgery-workflow-paper/" target="_blank">press coverage</a>]
		</p>

		<li>
			<a href="https://arxiv.org/abs/2203.15251" target="_blank">Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation</a><br>
			<b>Yueming Jin</b>, Yang Yu, Cheng Chen, Zixu Zhao, Pheng-Ann Heng, Danail Stoyanov. <br>
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>TMI</b></i>), 2022. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/YuemingJin/STswinCL" target="_blank">code</a>][<a href="./video/2022TMI.mp4" target="_blank">video</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2207.04655" target="_blank">Personalizing Federated Medical Image Segmentation via Local Calibration</a><br>
			Jiacheng Wang*, <b>Yueming Jin</b>*, Liansheng Wang. <br>
			<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2022. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/jcwang123/FedLC" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2103.16327" target="_blank">Temporal Memory Relation Network for Workflow Recognition from Surgical Video</a><br>
			<b>Yueming Jin</b>, Yonghao Long, Cheng Chen, Zixu Zhao, Qi Dou, Pheng-Ann Heng. <br>
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>TMI</b></i>), 2021. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/YuemingJin/TMRNet" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/2109.13499" target="_blank">Modelling Neighbor Relation in Joint Space-Time Graph for Video Correspondence Learning</a><br>
			Zixu Zhao, <b>Yueming Jin</b>†, Pheng-Ann Heng. <br>
			<em>International Conference on Computer Vision</em> (<i><b>ICCV</b></i>), 2021. <br> 
		</li>
			<p style="margin-top:3px">
				[<a href="./video/2021ICCV.mp4" target="_blank">video</a>][<a href="./video/iccv21_poster.pdf" target="_blank">poster</a>]
			</p>

		<li>
			<a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841521003364" target="_blank">Real-time Landmark Detection for Precise Endoscopic Submucosal Dissection via Shape-aware Relation Network</a><br>
			Jiacheng Wang*, <b>Yueming Jin</b>*, Shuntian Cai, Hongzhi Xu, Pheng-Ann Heng, Jing Qin, Liansheng Wang. <br>
			<em>Medical Image Analysis</em> (<i><b>MedIA</b></i>), 2021.<br> 
		</li>
			<p style="margin-top:3px">

			</p>

		<li>
			<a href="https://arxiv.org/abs/2103.12988" target="_blank">One to Many: Adaptive Instrument Segmentation via Meta Learning and Dynamic Online Adaptation in Robotic Surgical Video</a><br>
			Zixu Zhao, <b>Yueming Jin</b>†, Bo Lu, Chi-Fai Ng, Qi Dou, Yun-Hui Liu, Pheng-Ann Heng. <br>
			<em>IEEE International Conference on Robotics and Automation</em> (<i><b>ICRA</b></i>), 2021.<br> 
		</li>
			<p style="margin-top:3px">
				[<a href="./video/2021ICRA.mp4" target="_blank">video</a>]
			</p>

		<li>
			<a href="https://arxiv.org/abs/1907.06099" target="_blank">Multi-Task Recurrent Convolutional Network with Correlation Loss for Surgical Video Analysis</a><br>
			<b>Yueming Jin</b>, Huaxia Li, Qi Dou, Hao Chen, Jing Qin, Chi-Wing Fu, Pheng-Ann Heng. <br>
			<em>Medical Image Analysis</em> (<i><b>MedIA</b></i>), 2020.<br>
			<font color="red">(Ranked 1st place in MICCAI 2019 EndoVis Surgical Workflow Challenge on phase segmentation)</font><br>
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/YuemingJin/MTRCNet-CL" target="_blank">code</a>]
			</p>

	<li>
		<a href="https://arxiv.org/abs/1932.08718" target="_blank">Automatic Gesture Recognition in Robot-assisted Surgery with Reinforcement Learning and Tree Search</a><br>
		Xiaojie Gao, <b>Yueming Jin</b>, Qi Dou, Pheng-Ann Heng. <br>
		<em>International Conference on Robotics and Automation</em> (<i><b>ICRA</b></i>), 2020.<br>
	</li>
		<p style="margin-top:3px">

		</p>

		<li>
			<a href="https://arxiv.org/abs/1907.07899" target="_blank">Incorporating Temporal Prior from Motion Flow for Instrument Segmentation in Minimally Invasive Surgery Video</a><br>
			<b>Yueming Jin</b>, Keyun Cheng, Qi Dou, Pheng-Ann Heng. <br>
			<em>Medical Image Computing and Computer Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2019. <i><p style="color: red; display: inline;">(Oral)</p></i>
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/keyuncheng/MF-TAPNet" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://ieeexplore.ieee.org/document/8240734" target="_blank">SV-RCNet: Workflow Recognition from Surgical Videos using Recurrent Convolutional Network</a><br>
			<b>Yueming Jin</b>, Qi Dou, Hao Chen, Lequan Yu, Jing Qin, Chi-Wing Fu, Pheng-Ann Heng. <br>
			<em>IEEE Transactions on Medical Imaging</em> (<i><b>TMI</b></i>), 2018. <br>
			<font color="red">(Ranked 1st place in MICCAI M2CAI 2016 Surgical Workflow Challenge)</font><br> 
		</li>
			<p style="margin-top:3px">
				[<a href="https://github.com/YuemingJin/SV-RCNet" target="_blank">code</a>]
			</p>

		<li>
			<a href="https://dl.acm.org/citation.cfm?id=3130803" target="_blank">Reconfigurable Interlocking Furniture</a><br>
			Peng Song, Chi-Wing Fu, <b>Yueming Jin</b>, Hongfei Xu, Ligang Liu, Pheng-Ann Heng, Daniel Cohen-Or. <br>
			<em>ACM Transactions on Graphics</em> (<i><b>ACM TOG</b></i>), <i><b>SIGGRAPH Asia</b></i>, 2017. <br>
			<font >(Results were selected to be included in the technical papers trailer)</font><br>
		</li>
			<p style="margin-top:3px">
				[<a href="http://www.cse.cuhk.edu.hk/~cwfu/papers/reconfig/index.htm" target="_blank">project</a>]
			</p>

		<li>
			<a href="https://link.springer.com/chapter/10.1007/978-3-319-46723-8_18" target="_blank">3D Deeply Supervised Network for Automatic Liver Segmentation from CT Volumes</a><br>
			Qi Dou, Hao Chen, <b>Yueming Jin</b>, Lequan Yu, Jing Qin, Pheng-Ann Heng. <br>
			<em>Medical Image Computing and Computer Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2016. <br> 
		<font color="red">(Young Scientist Award Finalist)</font><br>
		</li>
			<p style="margin-top:3px">
				
			</p>


	</ul>


</tbody></table>
* indicates equal contribution; † indicates corresponding authorship.


<h2>Selected Awards</h2>
<table style="border-spacing:2px">
	
		<tbody>
		<tr><td> <a href="https://www.forbes.com/profile/jin-yueming/?list=30under30-asia-healthcare-science/&sh=263521c16612">Forbes 30 Under 30 Asia List, Class 2024</a> [<a href="https://cde.nus.edu.sg/news-detail/cde-names-listed-on-forbes-30-under-30-list/" target="_blank">NUS press coverage</a>] </td></tr>
		<tr><td> <a href="https://miccai.org/index.php/about-miccai/awards/ijcars-best-paper-award/" target="_blank">1st Prize of Best Paper Award</a> of IJCARS-MICCAI 2021 [<a href="https://cde.nus.edu.sg/news-detail/asst-prof-jin-yueming-wins-first-prize-for-ai-in-surgery-workflow-paper/" target="_blank">NUS press coverage</a>]</td></tr>
		<tr><td> Best Paper Award in Medical Robotics at ICRA 2021</td></tr>
		<tr><td> <a href="http://www.cse.cuhk.edu.hk/en/achievement/professor-and-research-staff/110-prof-heng-pheng-ann-and-his-team-won-the-best-paper-award-of-medical-image-analysis-in-the-20th-international-conference-on-medical-image-computing-and-computer-assisted-intervention-2017-media-miccai-17">Best Paper Award of MedIA-MICCAI 2017 </a>, 2017</td></tr>
		<tr><td>Young Scientist Publication Impact Award Finalist at MICCAI, 2021</td></tr>
    	<tr><td>Young Scientist Award Runner Up at MICCAI, 2016 </td></tr>
    	<tr><td> NIH Awards at MICCAI 2020</td></tr>
		<tr><td> <a href="https://www.synapse.org/#!Synapse:syn25127311/wiki/" target="_blank">Champion, SimSurgSkill Challenge on Objective Surgical Skills Assessment (MICCAI 2021)</a> </td></tr>
		<tr><td> <a href="https://www.synapse.org/#!Synapse:syn18779624/wiki/">Champion, ROBUST-MIS Challenge on Multi-instance Segmentation (MICCAI 2019)</a> </td></tr>
		<tr><td> <a href="https://www.synapse.org/#!Synapse:syn18824884/wiki/">Champion, EndoVis Surgical Workflow Challenge on Phase Segmentation (MICCAI 2019)</a> </td></tr>
    	<tr><td> <a href="http://camma.u-strasbg.fr/m2cai2016/index.php/workflow-challenge-results/">Champion, Surgical Workflow Challenge (MICCAI M2CAI 2016)</a> </td></tr>
		<tr><td> Teaching Assistant of Merit, 2016</td></tr>
		<tr><td> Hong Kong PhD Fellowship, 2015-2019</td></tr>
    	<!--<tr><td> National Scholarship in China, 2013,2014</td></tr>
    	<tr><td> Outstanding Graduates of Northeastern University, 2015</td></tr>-->
	</tbody>
</table>

<h2>Team</h2>
	<b>Current Students:</b>
	<br>	
	<table style="border-spacing:2px">
	<tbody>
		<tr><td> Qian Li (B.E., Ph.D., HIT)(Research Fellow)</td></tr>
		<tr><td> <a href="https://scholar.google.com/citations?user=ZSmTNXEAAAAJ&hl=en" target="_blank"> Haofeng Liu</a> (M.S. SUSTech)(PhD Student, 2024 Spring)</td></tr>
		<tr><td> Shuojue Yang (M.S. JHU)(PhD Student, 2024 Spring)</td></tr>
		<tr><td> Yibing Fu (B.E. M.S. Beihang)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> Borui Kang (B.E. THU)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> Guanyi Qin (B.E. SCUT M.S. THU)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> Daiyun Shen (B.E. THU)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> Zhitao Zeng (B.E. Nankai M.S. CAS)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> <a href="https://scholar.google.com/citations?user=gfjYZKMAAAAJ&hl=en" target="_blank"> Erli Zhang</a> (B.E. NTU)(PhD Student, 2024 Fall)</td></tr>
		<tr><td> <a href="https://github.com/sagizty" target="_blank"> Tianyi Zhang</a> (B.E. Beihang M.S. NTU)(PhD Student, 2024 Fall)</td></tr>
		
		<tr><td> 	</td></tr>
		<tr><td> Mingxuan Hong (B.E. CUHK)(MEng Student, 2023-)</td></tr>
		<tr><td> Yunpeng Zhao (B.E. Beihang)(MEng Student, 2023-)</td></tr>
		<tr><td> Zheng Fang (B.E. ZJU-UIUC)(MEng Student, 2023-)</td></tr>
		<tr><td> Ziyue Wang (B.E. HIT-SZ)(MEng Student, 2024-)</td></tr>
		<tr><td> Chang Han Low (B.E. NUS)(MEng Student, 2024-)</td></tr>
		<tr><td> Bochong Zhang (B.E. Nankai)(MEng Student, 2024-)</td></tr>
		<tr><td> Zhu Zhuo (B.E. SUSTech)(MEng Student, 2024-)</td></tr>
		<tr><td> 	</td></tr>
		<tr><td> Exchange PhD students: Ziwei Niu (Ph.D. ZJU), Xihong Yang (Ph.D. NUDT)</td></tr>
		<tr><td> Exchange B.E. students: Jiaan Zhang (B.E. ZJU), Bian Shujun (B.E. ZJU)</td></tr>
	</tbody>
	</table>
	<br>	
	
	<b>Alumni:</b>
	<br>
	<table style="border-spacing:2px">
	<tbody>
	<tr><td> <a href="https://scholar.google.com/citations?user=FZSKG-AAAAAJ&hl=en" target="_blank"> Junde Wu</a> (2023-24 RA)(M.S. HIT -> PhD at Oxford)</td></tr>
	<tr><td> Zhimin Shao (2023 Summer Intern) (B.E. M.S. at THU -> PhD at JHU) </td></tr>
	<tr><td> KinHei Lee (2024 Intern), (B.E. at CUHK -> Mres at ICL)</td></tr>
	<tr><td> Exchange PhD students: Xiaoming Qi (Ph.D. SEU), Shi Yin (Ph.D. CSU)</td></tr>
	</tbody>
	</table>
	<br>

	<b>Previous Mentees:</b>
	<br>
	<table style="border-spacing:2px">
	<tbody>
	<tr><td> Dimitrios Anastasiou (Ph.D. UCL)</td></tr>
	<tr><td> <a href="https://scholar.google.com.hk/citations?user=GSQY0CEAAAAJ&hl=zh-CN" target="_blank">Zixu Zhao</a> (2022 Ph.D. CUHK), Current: Amazon </td></tr>
	<tr><td> Xueying Shi (2021 Ph.D. CUHK), Current: Huawei</td></tr>
	</tbody>
	</table>
<br>


<h2>Professional Services</h2>

<li>	
	<b>Conference Services:</b>
    <br> * Area Chair of MICCAI 2023 2024, IPCAI 2023 2025
    <br> * Special Programmes Co-Chair of <a href="https://icbme2024.org/" target="_blank"> ICBME 2024</a>, Singapore
    <br> * Publicity Co-Chair of IEEE-RAS RoboSoft 2023
    <br> * Oral Session Chair of MICCAI 2024, ICRA 2021
    <br>* Organization Committee: <br>
    IJCAI Workshop on Trustworthy Machine Learning in Healthcare, 2024<br>
    ICLR Workshop on Trustworthy Machine Learning in Healthcare, 2023<br>
    MICCAI Workshop on Cancer Prevention through early detecTion, 2022, 2023, 2024<br>
	MICCAI Challenge on 3D Reconstruction During Colonoscopy, 2022<br>
	MICCAI Challenge on Pre-operative to Intra-operative Laparoscopy Fusion, 2022
	<br>* Regular Conference reviewer: <br>
	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<br>
	IEEE International Conference on Computer Vision (ICCV)<br>
	European Conference on Computer Vision (ECCV)<br>
	IEEE International Conference on Robotics and Automation (ICRA)<br>
	IEEE International Conference on Intelligent Robots and Systems (IROS)<br>
	Association for the Advancement of Artificial Intelligence (AAAI)<br>
	Medical Image Computing and Computer Assisted Intervention (MICCAI)<br>
	ACM International Conference on Multimedia (ACM MM)<br>
	<p style="margin-top:3px"></p>		
</li>

<li>
    <b>Regular Journal Reviewer:</b>
	<br>
	IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br>
	IEEE Transactions on Medical Imaging (TMI)<br>
	Medical Image Analysis (MedIA)<br>
	Nature Communications Medicine<br>
	International Journal of Surgery <br>
	IEEE Journal of Biomedical and Health Informatics (JBHI)<br>
	IEEE Robotics and Automation Letters (RA-L)<br>
	Medical Physics <br>
	Neurocomputing <br>	
	International Journal of Computer Assisted Radiology and Surgery (IJCARS)<br>
	Computer Methods and Programs in Biomedicine (CMPB)<br>
	<p style="margin-top:3px"></p>		
</li>

<h2>Selected Talks</h2>
<li> Invited talk about VLM-assisted medical intelligence at Global Initiative on AI for Health (GI-AI4H) <br> with World Health Organization (WHO), Singapore, 2025.
</li>
<li>Invited talk about intelligent surgical vision at VALSE Webinar, China, Jun. 2024 <a href="https://www.bilibili.com/video/BV1hw4m1Y7pf/?spm_id_from=333.1007.top_right_bar_window_dynamic.content.click&vd_source=fd5f237755b930fc4af6d6bf0bb3b9bc" target="_blank">[Record in Chinese]</a>
</li>
<li>Towards AI-powered Robotic Surgical Vision.<br>
Woman in Medical Image Understanding and Analysis, University of Cambridge, UK, Jul. 2022.
</li>
<li>Towards AI-powered Robotic Surgical Vision.<br>
IUPESM World Congress, Singapore, Jun. 2022.
</li>
<li>Dynamic Surgical Video Analysis for Intelligent Robotic Surgery.<br>
CMIC Seminar at University College London (UCL),  Sep. 2021.
</li>
<li>Dynamic Surgical Video Analysis for Intelligent Robotic Surgery.<br>
The Hong Kong Polytechnic University (PolyU),  Nov. 2020.
</li>
<li>Towards Intelligent Surgery: Dynamic Surgical Visual Perception with Deep Learning.<br>
Theme-based Research Scheme Symposium on Image-guided Robotic Surgery, CUHK,  Nov. 2019.
</li>
<li>Towards Intelligent Surgery: Dynamic Surgical Video Analysis with Deep Learning.<br>
The 5th World Chinese Microbial Ecology and Intelligent Endoscopy Summit, Xiamen, Nov. 2019.
</li>

<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
	<tbody>
		<tr>
			<td> 2025</td><td>Spring</td><td>EE2211 Introduction to Machine Learning</td>
		</tr>
		<tr>
			<td> 2024</td><td>Fall</td><td>BN5211 Medical Robotic Intelligence</td>
		</tr>
		<tr>
			<td> 2024</td><td>Fall</td><td>BN5212 Advanced Machine Learning for Biomedical Engineering and Sciences</td>
		</tr>
		<tr>
			<td> 2024</td><td>Spring</td><td>EE2211 Introduction to Machine Learning</td>
		</tr>
		<tr>
			<td> 2023</td><td>Fall</td><td>BN5211 Medical Robotics</td>
		</tr>
		<!--
		<td><tr> </td></tr>
		<tr><td> TA at CUHK: </td></tr>
		<tr>
			<td> 2015</td><td>Fall</td><td>ESTR1934 Digital Logic and Systems</td>
		</tr>
		<tr>
			<td> 2016</td><td>Spring</td><td>CSCI1930 Data Structures</td>
		</tr>
		<tr>
			<td> 2016 </td><td>Fall</td><td>CSCI3260 Principles of Computer Graphics</td>
		</tr>
        	<tr>
			<td> 2017</td><td>Spring</td><td>CSCI1930 Data Structures</td>
		</tr>
		<tr>
			<td> 2017</td><td>Fall</td><td>CSCI3260 Principles of Computer Graphics</td>
		</tr> -->
	</tbody>
</table>



<div id="footer">
	<div id="footer-text"></div>
</div>
	<p><center>
      	<div id="clustrmaps-widget" style="width:10%">
      		<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=va-L6lHdAAU-pnGRWuo4n-3yc0GsyFLqhzS0NLNX8Lk"></script>
      		<noscript><a href='https://clustrmaps.com/site/xfn5'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=pT8r_ZMBBdBPTv7KnlTCiBDylmHyi1qsWdPpY_tIlqY'/></a></noscript>


	</div>        
	<br>
        &copy; Yueming Jin | Last updated: Aug 2024
     
      </center></p>


</div>
</body></html>
